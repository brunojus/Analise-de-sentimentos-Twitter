mycorpus <- tm_map(mycorpus, removeWords, stopwords('pt'))
wordcloud(mycorpus, min.freq=3, rot.per=0.5, scale=c(4,1),
random.color=T, max.word=45, random.order=F, colors=col)
auxCorpus <- tm_map(auxCorpus, removeWords, c("adcomunicacao","php","www","http","https","bit","nao","sobre")
)
mycorpus <- Corpus(VectorSource(auxCorpus))
wordcloud(mycorpus, min.freq=3, rot.per=0.5, scale=c(4,1),
random.color=T, max.word=45, random.order=F, colors=col)
source('~/tcc/script.r')
stopwords <- read.csv("~/tcc/stopwords.txt", sep="")
View(stopwords)
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords)
library(tm)
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords)
auxCorpus <- join$message
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords)
auxCorpus <- Corpus(VectorSource(join$message))
auxCorpus <- tm_map(auxCorpus, removeNumbers)
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords)
custom_stopwords <- c(stopwords, stopwords())
auxCorpus <- tm_map(auxCorpus, removeWords, custom_stopwords)
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords)
auxCorpus <- tm_map(auxCorpus, removeWords, "/home/bruno/tcc/stopwords.txt")
View(auxCorpus)
View(auxCorpus)
View(auxCorpus)
View(auxCorpus)
View(auxCorpus)
auxCorpus <- tm_map(auxCorpus, removeWords, c("adcomunicacao","php","www","http","https","bit","nao","sobre"))
View(auxCorpus)
auxCorpus <- tm_map(auxCorpus, PlainTextDocument)
library("wordcloud")
col <- brewer.pal(5,"Dark2")
wordcloud(auxCorpus, min.freq=3, rot.per=0.5, scale=c(4,1),
random.color=T, max.word=45, random.order=F, colors=col)
auxCorpus <- tm_map(auxCorpus, PlainTextDocument)
View(auxCorpus)
wordcloud(auxCorpus,max.words=50,colors=c("blue","red"))
auxCorpus <- tm_map(auxCorpus, stemDocument)
wordcloud(auxCorpus,max.words=50,colors=c("blue","red"))
auxCorpus <- Corpus(VectorSource(auxCorpus))
wordcloud(auxCorpus,max.words=50,colors=c("blue","red"))
auxCorpus <- Corpus(VectorSource(join$message))
auxCorpus <- tm_map(auxCorpus, removeNumbers)
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords('pt'))
auxCorpus <- tm_map(auxCorpus, removeWords, c("foi", "grand", "esta", "cidad", "tem", "ser", "nossa", "nosso", "pela", "nos", "sao", "das", "pelo", "hoje", "tambem","uma","com","adcomunicacao","php","www","http","https","bit","nao","sobre"))
auxCorpus <- tm_map(auxCorpus, removeWords, "/home/bruno/tcc/stopwords.txt")
auxCorpus <- Corpus(VectorSource(auxCorpus))
auxCorpus <- tm_map(auxCorpus, PlainTextDocument)
library("wordcloud")
col <- brewer.pal(5,"Dark2")
wordcloud(auxCorpus,max.words=50,colors=c("blue","red"))
auxCorpus <- Corpus(VectorSource(auxCorpus))
wordcloud(auxCorpus,max.words=50,colors=c("blue","red"))
wordcloud(auxCorpus,max.words=20,colors=c("blue","red"))
auxCorpus <- Corpus(VectorSource(join$message))
auxCorpus <- tm_map(auxCorpus, removeNumbers)
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords('pt'))
auxCorpus <- tm_map(auxCorpus, removeWords, c("spnoticias" , "federal", "todos", "todas", "lenoticia","foi", "grand", "esta", "cidad", "tem", "ser", "nossa", "nosso", "pela", "nos", "sao", "das", "pelo", "hoje", "tambem","uma","com","adcomunicacao","php","www","http","https","bit","nao","sobre"))
auxCorpus <- tm_map(auxCorpus, removeWords, "/home/bruno/tcc/stopwords.txt")
auxCorpus <- Corpus(VectorSource(auxCorpus))
auxCorpus <- tm_map(auxCorpus, stemDocument)
wordcloud(auxCorpus,max.words=50,colors=c("blue","red"))
wordcloud(auxCorpus,max.words=10,colors=c("blue","red"))
wordcloud(auxCorpus, max.words =100,min.freq=3,scale=c(4,.5),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
wordcloud(auxCorpus, max.words =10,min.freq=3,scale=c(4,.5),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
wordcloud(auxCorpus, max.words =10,min.freq=3,scale=c(4,.3),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
wordcloud(auxCorpus, max.words =10,min.freq=3,scale=c(2,.3),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
wordcloud(auxCorpus, max.words =10,min.freq=3,scale=c(2,1),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
auxCorpus <- Corpus(VectorSource(join$message))
auxCorpus <- tm_map(auxCorpus, removeNumbers)
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords('pt'))
auxCorpus <- tm_map(auxCorpus, removeWords, c("paulo","spnoticias" , "federal", "todos", "todas", "lenoticia","foi", "grand", "esta", "cidad", "tem", "ser", "nossa", "nosso", "pela", "nos", "sao", "das", "pelo", "hoje", "tambem","uma","com","adcomunicacao","php","www","http","https","bit","nao","sobre"))
auxCorpus <- tm_map(auxCorpus, removeWords, "/home/bruno/tcc/stopwords.txt")
auxCorpus <- Corpus(VectorSource(auxCorpus))
wordcloud(auxCorpus, max.words =50,min.freq=3,scale=c(2,1),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
auxCorpus <- Corpus(VectorSource(join$message))
auxCorpus <- tm_map(auxCorpus, removeNumbers)
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords('pt'))
auxCorpus <- tm_map(auxCorpus, removeWords, c("fazeraqui", "agora", "stuckert", "ano", "sera", "ate", "agora", "paulo","spnoticias" , "federal", "todos", "todas", "lenoticia","foi", "grand", "esta", "cidad", "tem", "ser", "nossa", "nosso", "pela", "nos", "sao", "das", "pelo", "hoje", "tambem","uma","com","adcomunicacao","php","www","http","https","bit","nao","sobre"))
auxCorpus <- tm_map(auxCorpus, removeWords, "/home/bruno/tcc/stopwords.txt")
auxCorpus <- Corpus(VectorSource(auxCorpus))
wordcloud(auxCorpus, max.words =50,min.freq=3,scale=c(2,1),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
auxCorpus <- Corpus(VectorSource(join$message))
auxCorpus <- tm_map(auxCorpus, removeNumbers)
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords('pt'))
auxCorpus <- tm_map(auxCorpus, removeWords, c("nesta", "mil", "fazer", "ainda", "pode", "bom", "fazeraqui", "agora", "stuckert", "ano", "sera", "ate", "agora", "paulo","spnoticias" , "federal", "todos", "todas", "lenoticia","foi", "grand", "esta", "cidad", "tem", "ser", "nossa", "nosso", "pela", "nos", "sao", "das", "pelo", "hoje", "tambem","uma","com","adcomunicacao","php","www","http","https","bit","nao","sobre"))
auxCorpus <- tm_map(auxCorpus, removeWords, "/home/bruno/tcc/stopwords.txt")
auxCorpus <- Corpus(VectorSource(auxCorpus))
wordcloud(auxCorpus, max.words =50,min.freq=3,scale=c(2,1),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
auxCorpus <- Corpus(VectorSource(join$message))
auxCorpus <- tm_map(auxCorpus, removeNumbers)
auxCorpus <- tm_map(auxCorpus, removeWords, stopwords('pt'))
auxCorpus <- tm_map(auxCorpus, removeWords, c("nfoto", "aqui", "anos", "vai", "contra", "estao", "nesta", "mil", "fazer", "ainda", "pode", "bom", "fazeraqui", "agora", "stuckert", "ano", "sera", "ate", "agora", "paulo","spnoticias" , "federal", "todos", "todas", "lenoticia","foi", "grand", "esta", "cidad", "tem", "ser", "nossa", "nosso", "pela", "nos", "sao", "das", "pelo", "hoje", "tambem","uma","com","adcomunicacao","php","www","http","https","bit","nao","sobre"))
auxCorpus <- tm_map(auxCorpus, removeWords, "/home/bruno/tcc/stopwords.txt")
auxCorpus <- Corpus(VectorSource(auxCorpus))
auxCorpus <- tm_map(auxCorpus, removeWords, c("nfoto", "aqui", "anos", "vai", "contra", "estao", "nesta", "mil", "fazer", "ainda", "pode", "bom", "fazeraqui", "agora", "stuckert", "ano", "sera", "ate", "agora", "paulo","spnoticias" , "federal", "todos", "todas", "lenoticia","foi", "grand", "esta", "cidad", "tem", "ser", "nossa", "nosso", "pela", "nos", "sao", "das", "pelo", "hoje", "tambem","uma","com","adcomunicacao","php","www","http","https","bit","nao","sobre"))
auxCorpus <- tm_map(auxCorpus, removeWords, "/home/bruno/tcc/stopwords.txt")
auxCorpus <- Corpus(VectorSource(auxCorpus))
wordcloud(auxCorpus, max.words =50,min.freq=3,scale=c(2,1),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
auxCorpus <- tm_map(auxCorpus, removeWords, c("nfoto", "aqui", "anos", "vai", "contra", "estao", "nesta", "mil", "fazer", "ainda", "pode", "bom", "fazeraqui", "agora", "stuckert", "ano", "sera", "ate", "agora", "paulo","spnoticias" , "federal", "todos", "todas", "lenoticia","foi", "grand", "esta", "cidad", "tem", "ser", "nossa", "nosso", "pela", "nos", "sao", "das", "pelo", "hoje", "tambem","uma","com","adcomunicacao","php","www","http","https","bit","nao","sobre","ricardo"))
wordcloud(auxCorpus, max.words =50,min.freq=3,scale=c(2,1),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
auxCorpus <- tm_map(auxCorpus, removeWords, c("nfoto", "aqui", "anos", "vai", "contra", "estao", "nesta", "mil", "fazer", "ainda", "pode", "bom", "fazeraqui", "agora", "stuckert", "ano", "sera", "ate", "agora", "paulo","spnoticias" , "federal", "todos", "todas", "lenoticia","foi", "grand", "esta", "cidad", "tem", "ser", "nossa", "nosso", "pela", "nos", "sao", "das", "pelo", "hoje", "tambem","uma","com","adcomunicacao","php","www","http","https","bit","nao","sobre","ricardo","vamos"))
wordcloud(auxCorpus, max.words =50,min.freq=3,scale=c(2,1),
random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())
save.image("~/tcc/src/teste.RData")
join$message <- auxCorpus
View(join)
View(join)
load("~/tcc/src/teste.RData")
View(auxCorpus)
source('~/tcc/src/geo.R')
# A função ipak instala e carrega multiplos pacotes no R.
# Ela checa se os pacotes estão instalados, instalas os que não estão, depois carrega e informa o status de todos os pacotes
# criando a função ipak:
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
source('~/tcc/src/geo.R')
source('~/tcc/src/geo.R')
source('~/tcc/src/geo.R')
appname <- "Geo-TWITTER-UNB"
## api key (examplo fictício abaixo)
key <- "D0mY6pBzGIW10JBltw8k7nVqC"
## api secret (examplo fictício abaixo)
secret <- "	km4PajAJXZDrPM0TVl92P2MMQYZ0KvcoUI5nHGmTD8Sr4it6CW"
# criando um token chamado "twitter_token"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
source('~/tcc/src/geo.R')
appname <- "Text Mining for cryptos"
## api key (examplo fictício abaixo)
key <- "H3lOJNG2EzkVEAqBxbSlaq61Z"
## api secret (examplo fictício abaixo)
secret <- "DcgjAZFYw1tPJo4HzdDRWbj1aUwCg1jJ682UhH0ZLChovd4mzT"
# criando um token chamado "twitter_token"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
# criando um token chamado "twitter_token"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
usuario <- "jairbolsonaro"
# Baixando os dados desse usuário
dados_usuario <- lookup_users(usuario)
followers_usuario <-get_followers(usuario, n = dados_usuario$followers_count, retryonratelimit = TRUE)
# buscando dados dos seguidores
dados_usuario_followers <- lookup_users(followers_usuario$user_id)
followers_usuario <-get_followers(usuario, n = dados_usuario$followers_count, retryonratelimit = TRUE)
length(followers_usuario$user_id)
# buscando dados dos seguidores
dados_usuario_followers <- lookup_users(followers_usuario$user_id)
head(dados_usuario_followers$location, 10)
dados_usuario_followers <-subset(dados_usuario_followers, location!="")
head(dados_usuario_followers$location, 10)
write.csv2(followers_usuario,"seguidoresBolsonaro.csv",row.names=FALSE)
write.csv2(followers_usuario,"/home/bruno/tcc/src/seguidoresBolsonaro.csv",row.names=FALSE)
geocode_results <- dados_usuario_followers[1:length(dados_usuario_followers$user_id),c(3,4)]
pb <- txtProgressBar(min = 0, max = nrow(geocode_results), style = 3)
for(i in 1:nrow(geocode_results)){
#print("Buscando...")
result <- geocode(dados_usuario_followers$location[i], output = "latlona", source = "dsk")
geocode_results$lat[i] <- as.numeric(result[2])
geocode_results$lon[i] <- as.numeric(result[1])
Sys.sleep(0.1)
setTxtProgressBar(pb, i)
}
View(geocode_results)
View(geocode_results)
View(geocode_results)
View(geocode_results)
View(geocode_results)
View(geocode_results)
close(pb)
bio_html <- as.data.frame(paste("<b> <center> <img src='", dados_usuario_followers$profile_image_url, "' height='64'' width='64' > </center> </b>
<center> <a href='http://www.twitter.com/",dados_usuario_followers$screen_name,"' target='_blank' >",
dados_usuario_followers$name,"</a> </center>",
"<p> <center> Localização: <br>",dados_usuario_followers$location,"</center> </p>", sep = "" ))
names(bio_html) <- "bio"
geocode_data <- cbind.data.frame(geocode_results,bio_html)
geocode_data %>%
leaflet() %>%                                       #carrega o leaflet
addTiles() %>%                                      #adiciona as camadas de mapas de acordo com o zoom
addMarkers(lng = ~lon, lat = ~lat,popup=~bio,     #mapeia a base de dados de acordo com as respectivas lat e lon
clusterOptions = markerClusterOptions())
save.image("~/Bolsonaro.RData")
source('~/tcc/src/geo.R')
# Implementando os pacotes que serão usados
packages <- c("rtweet", "dplyr", "ggmap","leaflet", "leaflet.extras")
source('~/tcc/src/geo.R')
source('~/tcc/src/geo.R')
usuario <- "cirogomes"
# Baixando os dados desse usuário
dados_usuario <- lookup_users(usuario)
followers_usuario <-get_followers(usuario, n = dados_usuario$followers_count, retryonratelimit = TRUE)
# Observando a quantidade de dados de seguidores que foi buscado
length(followers_usuario$user_id)
# buscando dados dos seguidores
dados_usuario_followers <- lookup_users(followers_usuario$user_id)
#verificando as primeiras 10 localizações buscadas no passo acima
head(dados_usuario_followers$location, 10)
followers_usuario <-get_followers(usuario, n = dados_usuario$followers_count, retryonratelimit = TRUE)
dados_usuario_followers <-subset(dados_usuario_followers, location!="")
geocode_results <- dados_usuario_followers[1:length(dados_usuario_followers$user_id),c(3,4)]
View(geocode_results)
dados_usuario_followers <-subset(dados_usuario_followers, location=="M")
View(geocode_results)
geocode_results <-subset(geocode_results, location=="M")
geocode_results <- dados_usuario_followers[1:length(dados_usuario_followers$user_id),c(3,4)]
dados_usuario_followers <-subset(dados_usuario_followers, location!="")
dados_usuario_followers <- lookup_users(followers_usuario$user_id)
dados_usuario_followers <- lookup_users(followers_usuario$user_id)
dados_usuario_followers <-subset(dados_usuario_followers, location!="")
dados_usuario_followers <-subset(dados_usuario_followers, location!="M")
View(dados_usuario_followers)
dados_usuario_followers <-subset(dados_usuario_followers, location!="61")
dados_usuario_followers <-subset(dados_usuario_followers, location!=".")
dados_usuario_followers <-subset(dados_usuario_followers, location!=" ")
geocode_results <- dados_usuario_followers[1:length(dados_usuario_followers$user_id),c(3,4)]
pb <- txtProgressBar(min = 0, max = nrow(geocode_results), style = 3)
save.image("~/tcc/src/ciro.RData")
source('~/tcc/src/geo.R')
load("~/tcc/src/Alckmin.RData")
geocode_data %>%
leaflet() %>%                                       #carrega o leaflet
addTiles() %>%                                      #adiciona as camadas de mapas de acordo com o zoom
addMarkers(lng = ~lon, lat = ~lat,popup=~bio,     #mapeia a base de dados de acordo com as respectivas lat e lon
clusterOptions = markerClusterOptions())
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
packages <- c("rtweet", "dplyr", "ggmap","leaflet", "leaflet.extras")
ipak(packages)
geocode_data %>%
leaflet() %>%                                       #carrega o leaflet
addTiles() %>%                                      #adiciona as camadas de mapas de acordo com o zoom
addMarkers(lng = ~lon, lat = ~lat,popup=~bio,     #mapeia a base de dados de acordo com as respectivas lat e lon
clusterOptions = markerClusterOptions())7
View(bio_html)
View(geocode_data)
View(geocode_results)
View(geocode_data)
geocode_data %>%
leaflet() %>%
addTiles() %>%
addMarkers(lng = ~lon, lat = ~lat,popup=~bio,     #mapeia a base de dados de acordo com as respectivas lat e lon
clusterOptions = markerClusterOptions())
View(followers_usuario)
load("~/tcc/src/Doria.RData")
geocode_data %>%
leaflet() %>%
addTiles() %>%
addMarkers(lng = ~lon, lat = ~lat,popup=~bio,
clusterOptions = markerClusterOptions())
devtools::install_github("sillasgonzaga/lexiconPT")
install.packages(c('devtools','curl'))
library('devtools')
devtools::install_github("sillasgonzaga/lexiconPT")
data("sentiLex_lem_PT02")
get_word_sentiment("temer")
library(lexiconPT)
get_word_sentiment("temer")
teste = get_word_sentiment("temer")
View(teste)
teste = get_word_sentiment("filho da puta")
View(teste)
teste = get_word_sentiment("filho da puta")
View(teste)
teste = get_word_sentiment("não vou ir hoje")
View(teste)
teste = get_word_sentiment("hoje")
View(teste)
View(teste)
teste = get_word_sentiment("ou")
View(teste)
teste = get_word_sentiment("hoje")
View(teste)
teste = get_word_sentiment("cu")
View(teste)
teste = get_word_sentiment("filho da puta")
View(teste)
teste[["oplexicon_v2.1"]]
teste = get_word_sentiment("fora Dilma")
View(teste)
teste = get_word_sentiment("Porra")
View(teste)
View(teste)
teste = get_word_sentiment("Bom dia")
View(teste)
View(teste)
teste = get_word_sentiment("Eu te amo ")
View(teste)
library(tidyverse) # pq nao da pra viver sem
library(ggExtra)
library(magrittr) # <3
library(lubridate)
library(stringr) # essencial para trabalhar com textos
library(tidytext) # um dos melhores pacotes para text mining
library(lexiconPT)
install.packages("date")
library(tidyverse) # pq nao da pra viver sem
library(ggExtra)
library(magrittr) # <3
library(lubridate)
library(stringr) # essencial para trabalhar com textos
library(tidytext) # um dos melhores pacotes para text mining
library(lexiconPT)
data("oplexicon_v3.0")
data("sentiLex_lem_PT02")
op30 <- oplexicon_v3.0
sent <- sentiLex_lem_PT02
df_comments <- read.delim("~/Documents/artigo/R/Aecio1_oplexiconLimpo.txt", stringsAsFactors=FALSE)
library(tidyverse) # pq nao da pra viver sem
library(ggExtra)
library(magrittr) # <3
library(lubridate)
library(stringr) # essencial para trabalhar com textos
library(tidytext) # um dos melhores pacotes para text mining
library(lexiconPT)
data("oplexicon_v3.0")
data("sentiLex_lem_PT02")
op30 <- oplexicon_v3.0
sent <- sentiLex_lem_PT02
df_comments <- read.delim("~/Documents/artigo/R/Aecio1_oplexiconLimpo.txt", stringsAsFactors=FALSE)
df_comments %<>% mutate(comment_id = row_number())
View(sentiLex_lem_PT02)
df_comments_unnested %>%
select(comment_id, term) %>%
head(20)
df_comments_unnested <- df_comments %>% unnest_tokens(term, message)
df_comments_unnested <- df_comments %>% unnest_tokens(term, text)
df_comments_unnested %>%
select(comment_id, term) %>%
head(20)
df_comments_unnested %>%
left_join(op30, by = "term") %>%
left_join(sent %>% select(term, lex_polarity = polarity), by = "term") %>%
select(comment_id, term, polarity, lex_polarity) %>%
head(10)
df_comments_unnested <- df_comments_unnested %>%
inner_join(op30, by = "term") %>%
inner_join(sent %>% select(term, lex_polarity = polarity), by = "term") %>%
group_by(comment_id) %>%
summarise(
comment_sentiment_op = sum(polarity),
comment_sentiment_lex = sum(lex_polarity),
n_words = n()
) %>%
ungroup() %>%
rowwise() %>%
mutate(
most_neg = min(comment_sentiment_lex, comment_sentiment_op),
most_pos = max(comment_sentiment_lex, comment_sentiment_op)
)
head(df_comments_unnested)
p <- df_comments_unnested %>%
ggplot(aes(x = comment_sentiment_op, y = comment_sentiment_lex)) +
geom_point(aes(color = n_words)) +
scale_color_continuous(low = "green", high = "red") +
labs(x = "Polaridade no OpLexicon", y = "Polaridade no SentiLex") +
#geom_smooth(method = "lm") +
geom_vline(xintercept = 0, linetype = "dashed") +
geom_hline(yintercept = 0, linetype = "dashed")
p
df_comments_unnested %<>% filter(between(comment_sentiment_op, -10, 10))
# comentario mais positivo da historia do sensacionalista
most_pos <- which.max(df_comments_unnested$most_pos)
most_neg <- which.min(df_comments_unnested$most_neg)
# mais positivo
cat(df_comments$text[df_comments$comment_id == df_comments_unnested$comment_id[most_pos]])
cat(df_comments$text[df_comments$comment_id == df_comments_unnested$comment_id[most_neg]])
df_comments1 %<>% inner_join(
df_comments_unnested %>% select(comment_id, sentiment = comment_sentiment_op),
by = "comment_id"
)
df_comments %<>% inner_join(
df_comments_unnested %>% select(comment_id, sentiment = comment_sentiment_op),
by = "comment_id"
)
View(df_comments)
View(df_comments)
df_comments_wide <- df_comments %>%
# filtrar fora palavras neutras
filter(sentiment != 0) %>%
# converter numerico para categorico
mutate(sentiment = ifelse(sentiment < 0, "negativo", "positivo")) %>%
# agrupar os dados
count(data, post_link, post_type, sentiment) %>%
# converter para formato wide
spread(sentiment, n, fill = 0) %>%
mutate(sentimento = positivo - negativo) %>%
ungroup() %>%
arrange(data)
head(df_comments_wide) %>% knitr::kable()
df_comments %>%
# filtrar fora palavras neutras
filter(sentiment != 0) %>%
# converter numerico para categorico
mutate(sentiment = ifelse(sentiment < 0, "negativo", "positivo")) %>%
# agrupar os dados
count(data, post_link, post_type, sentiment) %>%
# converter para formato wide
spread(sentiment, n, fill = 0) %>%
mutate(sentimento = positivo - negativo) %>%
ungroup() %>%
arrange(data)
head(df_comments_wide) %>% knitr::kable()
df_comments_wide <- df_comments %>%
# filtrar fora palavras neutras
filter(sentiment != 0) %>%
# converter numerico para categorico
mutate(sentiment = ifelse(sentiment < 0, "negativo", "positivo")) %>%
# agrupar os dados
count(data, post_link, post_type, sentiment) %>%
# converter para formato wide
spread(sentiment, n, fill = 0) %>%
mutate(sentimento = positivo - negativo) %>%
ungroup() %>%
arrange(data)
df_comments_wide <- df_comments %>%
# filtrar fora palavras neutras
filter(sentiment != 0) %>%
# converter numerico para categorico
mutate(sentiment = ifelse(sentiment < 0, "negativo", "positivo")) %>%
# agrupar os dados
count(data, post_link, post_type, sentiment) %>%
# converter para formato wide
spread(sentiment, n, fill = 0) %>%
mutate(sentimento = positivo - negativo) %>%
ungroup() %>%
d
df_comments_wide <- df_comments %>%
# filtrar fora palavras neutras
filter(sentiment != 0) %>%
# converter numerico para categorico
mutate(sentiment = ifelse(sentiment < 0, "negativo", "positivo")) %>%
# agrupar os dados
count(data, post_link, post_type, sentiment) %>%
# converter para formato wide
spread(sentiment, n, fill = 0) %>%
mutate(sentimento = positivo - negativo) %>%
ungroup() %>%
arrange(date)
df_comments_wide <- df_comments %>%
# filtrar fora palavras neutras
filter(sentiment != 0) %>%
# converter numerico para categorico
mutate(sentiment = ifelse(sentiment < 0, "negativo", "positivo")) %>%
# agrupar os dados
count(date,sentiment) %>%
# converter para formato wide
spread(sentiment, n, fill = 0) %>%
mutate(sentimento = positivo - negativo) %>%
ungroup() %>%
arrange(date)
View(df_comments_wide)
df_comments %>%
mutate(index = row_number()) %>%
ggplot(aes(x = index, y = sentimento)) +
geom_col(aes(fill = post_type)) +
scale_y_continuous(breaks = seq(-60, 60, 20), limits = c(-60, 60)) +
labs(x = "Índice da publicação", y = "Sentimento",
fill = NULL, title = "Evolução do sentimento em publicações do Sensacionalista")
View(df_comments)
View(df_comments)
write.csv2(df_comments,"~/Documents/artigo/R/dataaecio.txt";sep="\t")
write.csv2(df_comments,"~/Documents/artigo/R/dataaecio.txt",sep="\t")
write.csv(df_comments,"~/Documents/artigo/R/dataaecio.txt",sep="\t")
write.table(df_comments, file = "~/Documents/artigo/R/dataaecio.txt",row.names=FALSE, na="",col.names=FALSE, sep="\t")
write.table(df_comments, file = "~/Documents/artigo/R/dataaecio.txt",row.names=FALSE, na="",col.names=FALSE, sep="\t",quote = FALSE)
View(df_comments)
write.table(df_comments, file = "~/Documents/artigo/R/dataaecio.txt",row.names=FALSE, na="",col.names=TRUE, sep="\t",quote = FALSE)
View(df_comments)
###################
"load data"
###################
setwd("/home/bruno/Documents/artigo/R/Sentiment/)
happy = readLines("./happy.txt")
sad = readLines("./sad.txt")
happy_test = readLines("./happy_test.txt")
sad_test = readLines("./sad_test.txt")
tweet = c(happy, sad)sdsd
tweet_test= c(happy_test, sad_test)
tweet_all = c(tweet, tweet_test)
sentiment = c(rep("happy", length(happy) ),
rep("sad", length(sad)))
sentiment_test = c(rep("happy", length(happy_test) ),
rep("sad", length(sad_test)))
sentiment_all = as.factor(c(sentiment, sentiment_test))
library(RTextTools)
setwd('/home/bruno/Documents/artigo/R/Sentiment/')
