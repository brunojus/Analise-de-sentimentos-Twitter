{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier, SklearnClassifier, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk import precision\n",
    "from nltk.metrics import *\n",
    "from nltk.metrics import spearman\n",
    "from nltk.metrics import paice\n",
    "from nltk.metrics import scores\n",
    "import csv\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpLexicon'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"OpLexicon\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posdata = []\n",
    "with open('/home/bruno/Documents/artigo/R/Sentiment/positive_op.csv', 'rt') as myfile:    \n",
    "    reader = csv.reader(myfile, delimiter='\\t')\n",
    "    for val in reader:\n",
    "        posdata.append(val[0])        \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neudata = []\n",
    "with open('/home/bruno/Documents/artigo/R/Sentiment/neutral_op.csv', 'rt') as myfile:    \n",
    "    reader = csv.reader(myfile, delimiter='\\t')\n",
    "    for val in reader:\n",
    "        neudata.append(val[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "negdata = []\n",
    "with open('/home/bruno/Documents/artigo/R/Sentiment/negative_op.csv', 'rt') as myfile:    \n",
    "    reader = csv.reader(myfile, delimiter='\\t')\n",
    "    for val in reader:\n",
    "        negdata.append(val[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_split(data):    \n",
    "    data_new = []\n",
    "    for word in data:\n",
    "        word_filter = [i.lower() for i in word.split()]\n",
    "        data_new.append(word_filter)\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_split_sentiment(data):\n",
    "    data_new = []\n",
    "    for (word, sentiment) in data:\n",
    "        word_filter = [i.lower() for i in word.split()]\n",
    "        data_new.append((word_filter, sentiment))\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):    \n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopset = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_filtered_word_feats(words):\n",
    "    return dict([(word, True) for word in words if word not in stopset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    \"\"\"\n",
    "    print words\n",
    "    for ngram in itertools.chain(words, bigrams): \n",
    "        if ngram not in stopset: \n",
    "            print ngram\n",
    "    exit()\n",
    "    \"\"\"    \n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_word_feats_stopwords(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    \"\"\"\n",
    "    print words\n",
    "    for ngram in itertools.chain(words, bigrams): \n",
    "        if ngram not in stopset: \n",
    "            print ngram\n",
    "    exit()\n",
    "    \"\"\"    \n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams) if ngram not in stopset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------\n",
      "SINGLE FOLD RESULT (Naive Bayes)\n",
      "---------------------------------------\n",
      "accuracy: 0.9310375432679506\n",
      "precision 0.9289425407804587\n",
      "recall 0.9208777864563791\n",
      "f-measure 0.9240368281723924\n",
      "\n",
      "---------------------------------------\n",
      "SINGLE FOLD RESULT (SVM)\n",
      "---------------------------------------\n",
      "accuracy: 0.9928108635839177\n",
      "precision 0.9928382723669756\n",
      "recall 0.9916277160626336\n",
      "f-measure 0.9922131115742525\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -1.09825        0.678\n",
      "         Final          -1.09788        0.678\n",
      "\n",
      "---------------------------------------\n",
      "SINGLE FOLD RESULT (Maximum Entropy)\n",
      "---------------------------------------\n",
      "accuracy: 0.6757788231117423\n",
      "precision 0.8376024255831954\n",
      "recall 0.5843311285349632\n",
      "f-measure 0.605458196076152\n",
      "\n",
      "---------------------------------------\n",
      "SINGLE FOLD RESULT (Decision Tree)\n",
      "---------------------------------------\n",
      "accuracy: 0.8515132688382\n",
      "precision 0.8905628689637295\n",
      "recall 0.8164230751402203\n",
      "f-measure 0.8422551664990241\n",
      "\n",
      "---------------------------------------\n",
      "N-FOLD CROSS VALIDATION RESULT (Naive Bayes)\n",
      "---------------------------------------\n",
      "accuracy: 0.9222814025743451\n",
      "precision 0.9216502434845536\n",
      "recall 0.9460928643945077\n",
      "f-measure 0.9139487976611272\n",
      "\n",
      "---------------------------------------\n",
      "N-FOLD CROSS VALIDATION RESULT (SVM)\n",
      "---------------------------------------\n",
      "accuracy: 0.990878828229028\n",
      "precision 0.9906622174656113\n",
      "recall 0.9943263086802187\n",
      "f-measure 0.9899703806993848\n",
      "\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -1.09825        0.675\n",
      "         Final          -1.09788        0.675\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -1.09825        0.674\n",
      "         Final          -1.09788        0.674\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -1.09825        0.674\n",
      "         Final          -1.09789        0.674\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.252\n",
      "             2          -1.09825        0.677\n",
      "         Final          -1.09788        0.677\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.252\n",
      "             2          -1.09825        0.676\n",
      "         Final          -1.09788        0.676\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.252\n",
      "             2          -1.09825        0.674\n",
      "         Final          -1.09788        0.674\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.252\n",
      "             2          -1.09825        0.678\n",
      "         Final          -1.09788        0.678\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -1.09824        0.675\n",
      "         Final          -1.09787        0.675\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.251\n",
      "             2          -1.09824        0.674\n",
      "         Final          -1.09788        0.674\n",
      "  ==> Training (3 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.250\n",
      "             2          -1.09825        0.676\n",
      "         Final          -1.09788        0.676\n",
      "---------------------------------------\n",
      "N-FOLD CROSS VALIDATION RESULT (Maximum Entropy)\n",
      "---------------------------------------\n",
      "accuracy: 0.6621615623612961\n",
      "precision 0.8382199593001288\n",
      "recall 0.806952936062758\n",
      "f-measure 0.5837488573798085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_classifier(featx):\n",
    "    \n",
    "    negfeats = [(featx(f), 'neg') for f in word_split(negdata)]\n",
    "    posfeats = [(featx(f), 'pos') for f in word_split(posdata)]\n",
    "    neufeats = [(featx(f), 'neu') for f in word_split(neudata)]    \n",
    "    \n",
    "    negcutoff = int(len(negfeats)*3/4)\n",
    "    poscutoff = int(len(posfeats)*3/4)\n",
    "    neucutoff = int(len(neufeats)*3/4)\n",
    "    \n",
    " \n",
    "    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff] + neufeats[:neucutoff]\n",
    "    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:] + neufeats[neucutoff:]\n",
    "    \n",
    "    # using 3 classifiers\n",
    "    classifier_list = ['nb', 'svm','maxent','decision_tree']     \n",
    "        \n",
    "    for cl in classifier_list:\n",
    "        \n",
    "        if cl == 'maxent':\n",
    "            classifierName = 'Maximum Entropy'\n",
    "            algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]\n",
    "            classifier = nltk.MaxentClassifier.train(trainfeats, algorithm,max_iter=3)\n",
    "        elif cl == 'svm':\n",
    "            classifierName = 'SVM'\n",
    "            classifier = SklearnClassifier(LinearSVC(), sparse=False)\n",
    "            classifier.train(trainfeats)\n",
    "            \n",
    "        elif cl == 'decision_tree':\n",
    "            classifierName = 'Decision Tree'\n",
    "            classifier = DecisionTreeClassifier.train(trainfeats, binary=True, depth_cutoff=20, support_cutoff=20, entropy_cutoff=0.01)\n",
    "            \n",
    "        else:\n",
    "            classifierName = 'Naive Bayes'\n",
    "            classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "            \n",
    "        refsets = collections.defaultdict(set)\n",
    "        testsets = collections.defaultdict(set)\n",
    " \n",
    "        for i, (feats, label) in enumerate(testfeats):\n",
    "                refsets[label].add(i)\n",
    "                observed = classifier.classify(feats)\n",
    "                testsets[observed].add(i)\n",
    " \n",
    "        accuracy = nltk.classify.util.accuracy(classifier, testfeats)\n",
    "    \n",
    "        pos_precision = scores.precision(refsets['pos'], testsets['pos'])\n",
    "        pos_recall = scores.recall(refsets['pos'], testsets['pos'])\n",
    "        pos_fmeasure = scores.f_measure(refsets['pos'], testsets['pos'])\n",
    "        \n",
    "        neg_precision = scores.precision(refsets['neg'], testsets['neg'])\n",
    "        neg_recall = scores.recall(refsets['neg'], testsets['neg'])\n",
    "        neg_fmeasure =  scores.f_measure(refsets['neg'], testsets['neg'])\n",
    "        \n",
    "        neu_precision = scores.precision(refsets['neu'], testsets['neu'])\n",
    "        neu_recall = scores.recall(refsets['neu'], testsets['neu'])\n",
    "        neu_fmeasure = scores.f_measure(refsets['neu'], testsets['neu'])\n",
    "        \n",
    "        print ('')\n",
    "        print ('---------------------------------------')\n",
    "        print ('SINGLE FOLD RESULT ' + '(' + classifierName + ')')\n",
    "        print ('---------------------------------------')\n",
    "        print ('accuracy:', accuracy)\n",
    "        print ('precision', (pos_precision + neg_precision + neu_precision) / 3)\n",
    "        print ('recall', (pos_recall + neg_recall + neu_recall) / 3)\n",
    "        print ('f-measure', (pos_fmeasure + neg_fmeasure + neu_fmeasure) / 3)    \n",
    "                \n",
    "        #classifier.show_most_informative_features()\n",
    "    \n",
    "    print ('')\n",
    "    \n",
    "    ## CROSS VALIDATION\n",
    "    \n",
    "    trainfeats = negfeats + posfeats + neufeats   \n",
    "    \n",
    "    # SHUFFLE TRAIN SET\n",
    "    # As in cross validation, the test chunk might have only negative or only positive data    \n",
    "    random.shuffle(trainfeats)    \n",
    "    n = 10 # 5-fold cross-validation    \n",
    "    \n",
    "    for cl in classifier_list:\n",
    "        \n",
    "        subset_size = int(len(trainfeats) / n)\n",
    "        accuracy = []\n",
    "        \n",
    "        pos_precision = []\n",
    "        pos_recall = []\n",
    "        pos_fmeasure = []\n",
    "        \n",
    "        neg_precision = []\n",
    "        neg_recall = []\n",
    "        neg_fmeasure = []\n",
    "        \n",
    "        neu_precision = []\n",
    "        neu_recall = []\n",
    "        neu_fmeasure = []\n",
    "\n",
    "        cv_count = 1\n",
    "        for i in range(n):        \n",
    "            testing_this_round = trainfeats[i*subset_size:][:subset_size]\n",
    "            training_this_round = trainfeats[:i*subset_size] + trainfeats[(i+1)*subset_size:]\n",
    "            \n",
    "            if cl == 'maxent':\n",
    "                classifierName = 'Maximum Entropy'\n",
    "                algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]\n",
    "                classifier = nltk.MaxentClassifier.train(training_this_round, algorithm,max_iter=3)\n",
    "            elif cl == 'svm':\n",
    "                classifierName = 'SVM'\n",
    "                classifier = SklearnClassifier(LinearSVC(), sparse=False)\n",
    "                classifier.train(training_this_round)\n",
    "            \n",
    "            elif cl == 'decision_tree':\n",
    "                classifierName = 'Decision Tree'\n",
    "                classifier = DecisionTreeClassifier.train(training_this_round, binary=True, depth_cutoff=20, support_cutoff=20, entropy_cutoff=0.01)\n",
    "            else:\n",
    "                classifierName = 'Naive Bayes'\n",
    "                classifier = NaiveBayesClassifier.train(training_this_round)\n",
    "                    \n",
    "            refsets = collections.defaultdict(set)\n",
    "            testsets = collections.defaultdict(set)\n",
    "            for i, (feats, label) in enumerate(testing_this_round):\n",
    "                refsets[label].add(i)\n",
    "                observed = classifier.classify(feats)\n",
    "                testsets[observed].add(i)\n",
    "            \n",
    "            cv_accuracy = nltk.classify.util.accuracy(classifier, testing_this_round)\n",
    "            cv_pos_precision = scores.precision(refsets['pos'], testsets['pos'])\n",
    "            cv_pos_recall = scores.recall(refsets['pos'], testsets['pos'])\n",
    "            cv_pos_fmeasure = scores.f_measure(refsets['pos'], testsets['pos'])\n",
    "            \n",
    "            cv_neg_precision = scores.precision(refsets['neg'], testsets['neg'])\n",
    "            cv_neg_recall = scores.recall(refsets['neg'], testsets['neg'])\n",
    "            cv_neg_fmeasure =  scores.f_measure(refsets['neg'], testsets['neg'])\n",
    "            \n",
    "            cv_neu_precision = scores.precision(refsets['neu'], testsets['neu'])\n",
    "            cv_neu_recall = scores.recall(refsets['neu'], testsets['neu'])\n",
    "            cv_neu_fmeasure = scores.f_measure(refsets['neu'], testsets['neu'])\n",
    "                    \n",
    "            accuracy.append(cv_accuracy)\n",
    "            pos_precision.append(cv_pos_precision)\n",
    "            pos_recall.append(cv_pos_recall)\n",
    "            pos_fmeasure.append(cv_pos_fmeasure)\n",
    "\n",
    "            neg_precision.append(cv_neg_precision)\n",
    "            neg_recall.append(cv_neg_recall)\n",
    "            neg_fmeasure.append(cv_neg_fmeasure)\n",
    "            \n",
    "            neu_precision.append(cv_neu_precision)\n",
    "            neu_recall.append(cv_neu_recall)\n",
    "            neu_fmeasure.append(cv_neu_fmeasure)\n",
    "            \n",
    "            cv_count += 1\n",
    "                \n",
    "        print ('---------------------------------------')\n",
    "        print ('N-FOLD CROSS VALIDATION RESULT ' + '(' + classifierName + ')')\n",
    "        print ('---------------------------------------')\n",
    "        print ('accuracy:', sum(accuracy) / n)\n",
    "        print ('precision', (sum(pos_precision)/n + sum(neg_precision)/n + sum(neu_precision)/n) / 3)\n",
    "        print ('recall',  (sum(pos_recall)/n + sum(neg_recall)/n + sum(neu_precision)/n) / 3)\n",
    "        print ('f-measure', (sum(pos_fmeasure)/n + sum(neg_fmeasure)/n + sum(neu_fmeasure)/n) / 3)\n",
    "        print ('')\n",
    "    \n",
    "        \n",
    "evaluate_classifier(word_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SklearnClassifier' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-15d97dbd7edc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-15d97dbd7edc>\u001b[0m in \u001b[0;36mevaluate_classifier\u001b[0;34m(featx)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SklearnClassifier' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "def evaluate_classifier(featx):\n",
    "    \n",
    "    negfeats = [(featx(f), 'neg') for f in word_split(negdata)]\n",
    "    posfeats = [(featx(f), 'pos') for f in word_split(posdata)]\n",
    "    neufeats = [(featx(f), 'neu') for f in word_split(neudata)]    \n",
    "    \n",
    "    negcutoff = int(len(negfeats)*3/4)\n",
    "    poscutoff = int(len(posfeats)*3/4)\n",
    "    neucutoff = int(len(neufeats)*3/4)\n",
    "    \n",
    " \n",
    "    trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff] + neufeats[:neucutoff]\n",
    "    testfeats = negfeats[negcutoff:] + posfeats[poscutoff:] + neufeats[neucutoff:]\n",
    "    \n",
    "    classifier = SklearnClassifier(LinearSVC(), sparse=False)\n",
    "    classifier.train(trainfeats)\n",
    "    \n",
    "    classifier.fit(refsets['pos'], testsets['pos'])\n",
    "\n",
    "evaluate_classifier(word_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1e52701ef5b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
